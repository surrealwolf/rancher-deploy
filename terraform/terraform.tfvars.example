# Rancher Deploy - Terraform Variables Example
# 
# INSTRUCTIONS:
# 1. Copy this file: cp terraform.tfvars.example terraform.tfvars
# 2. Update values for your environment
# 3. Never commit terraform.tfvars to git (it's in .gitignore)
# 4. For team sharing, keep only terraform.tfvars.example in git
#
# This configuration creates:
# - 3 Rancher Manager nodes (VMs 401-403)
# - 2 Application cluster nodes (VMs 404-405)
# - All from Ubuntu 24.04 LTS cloud images
#
# See CLOUD_IMAGE_SETUP.md for detailed instructions

# ============================================================================
# PROXMOX CONFIGURATION
# ============================================================================

proxmox_api_url = "https://192.168.1.10:8006"

# API user format: username@realm (typically root@pam for local auth)
proxmox_api_user = "root@pam"

# Token ID: Usually format is "tokenname" (not the full user!tokenid format)
proxmox_api_token_id = "terraform"

# Token secret: The actual token value (keep this secret!)
proxmox_api_token_secret = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Set to false in production with valid certificates
proxmox_tls_insecure = true

# Target Proxmox node where VMs will be created
proxmox_node = "pve"

# ============================================================================
# UBUNTU CLOUD IMAGE CONFIGURATION (24.04 LTS Noble)
# ============================================================================

# Ubuntu cloud image URL - change to different release if needed:
# - 24.04 (Focal): https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img
# - 22.04 (Jammy): https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img
# - 24.04 (Noble): https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img
ubuntu_cloud_image_url = "https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"

# ============================================================================
# CLUSTER CONFIGURATION
# ============================================================================

clusters = {
  manager = {
    name           = "rancher-manager"
    node_count     = 3
    cpu_cores      = 4               # 2-4 minimum for Rancher
    memory_mb      = 8192            # 4GB minimum, 8GB recommended
    disk_size_gb   = 50               # Optimized: 50GB sufficient (can expand later if needed)
    domain         = "example.com"  # DNS domain for VMs
    ip_subnet      = "192.168.1"    # Base subnet
    ip_start_octet = 100             # VMs will be .100, .101, .102
    gateway        = "192.168.1.1"  # Network gateway
    dns_servers    = ["192.168.1.1"]
    storage        = "local-vm-zfs" # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id        = 1               # VLAN ID (1 for no VLAN, or your VLAN number)
  }

  nprd-apps = {
    name           = "nprd-apps"
    node_count     = 3              # Server nodes (control plane + etcd)
    worker_count   = 0              # Worker nodes (set to 0 to disable, or 2-4 for hybrid setup)
    cpu_cores      = 4               # Server CPU cores (can be 2-8 depending on workload)
    memory_mb      = 8192           # Server memory (4GB minimum, 8GB recommended)
    disk_size_gb   = 50              # Optimized: 50GB sufficient for control plane + etcd
    # Worker node resources (optional - defaults to server values if not specified)
    worker_cpu_cores    = 0         # Worker CPU cores (0 = use server value, or specify 2-8)
    worker_memory_mb    = 0         # Worker memory (0 = use server value, or specify 4096-16384)
    worker_disk_size_gb = 40        # Optimized: 40GB sufficient for typical workloads (0 = use server value)
    domain         = "example.com"
    ip_subnet      = "192.168.1"   # Same subnet
    ip_start_octet = 110            # Server VMs: .110, .111, .112
                                    # Worker VMs: .113, .114, .115, .116 (if worker_count > 0)
    gateway        = "192.168.1.1"
    dns_servers    = ["192.168.1.1"]
    storage        = "local-vm-zfs" # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id        = 1              # VLAN ID (1 for no VLAN, or your VLAN number)
  }
}

# ============================================================================
# VM CONFIGURATION
# ============================================================================

# Starting VM IDs for clusters
vm_id_start_manager = 401  # Manager cluster: VMs 401-403
vm_id_start_apps    = 404  # Apps cluster: VMs 404-406

# ============================================================================
# CERT-MANAGER CONFIGURATION
# ============================================================================

cert_manager_version = "v1.13.0"

# Enable Rancher deployment (default: true)
# Rancher installs on manager cluster after it's ready, then apps cluster deploys
# Set to false to deploy manager and apps clusters without Rancher
install_rancher = true

rancher_version = "v2.13.1"

# Initial admin password (change immediately after first login!)
rancher_password = "change-me-to-secure-password"

# Rancher manager hostname (should be accessible from application clusters)
rancher_hostname = "rancher.example.com"

# ============================================================================
# DOWNSTREAM CLUSTER REGISTRATION
# ============================================================================

# Automatic Rancher registration for downstream cluster (nprd-apps)
# Set to true to automatically register cluster with Rancher Manager
# Required: rancher_api_token must be set for this to work
register_downstream_cluster = true

# Downstream cluster ID in Rancher (format: c-abc123)
# DEPRECATED: This is now automatically fetched from Rancher API
# The cluster ID is extracted after Rancher creates/imports the cluster
# You can find it in Rancher UI under Cluster Management, but it's no longer required here
# Kept for backward compatibility but no longer used
# downstream_cluster_id = ""  # Leave empty - automatically populated from Rancher API

# Rancher API token for cluster management
# Obtain from Rancher: Account → API Tokens → Create API Token
# Token format: token-xxxxx:secret
# IMPORTANT: Keep this secret! Store in tfvars (not in git)
rancher_api_token = "token-xxxxx:your-secret-token"

# ============================================================================
# SSH CONFIGURATION
# ============================================================================

# Path to SSH private key for VM access
# The key should match the public key configured in your Proxmox user account
ssh_private_key = "~/.ssh/id_rsa"

# ============================================================================
# DOWNSTREAM CLUSTER REGISTRATION
# ============================================================================

# Name of the downstream cluster to register with Rancher Manager
# Defaults to first non-manager cluster from clusters map (e.g., "nprd-apps")
# Change this to register a different cluster or leave empty for auto-detection
# downstream_cluster_name = "nprd-apps"  # Optional: specify explicitly

# Enable automatic registration of downstream cluster with Rancher Manager
# When true, Terraform will:
# 1. Create the cluster resource in Rancher automatically via API
# 2. Register all nodes with Rancher Manager automatically
# No manual Rancher UI steps required!
register_downstream_cluster = true

# ============================================================================
# TRUENAS / DEMOCRATIC CSI CONFIGURATION
# ============================================================================
# Configuration for democratic-csi storage driver with TrueNAS
# These values are used to generate Helm values for democratic-csi installation

# TrueNAS hostname or IP address
truenas_host = "tn.dataknife.net"

# TrueNAS API key (obtain from TrueNAS UI: System → API Keys → Add)
# Format: token-id-token-secret (e.g., 1-xxxxxxxxxxxxx)
truenas_api_key = "your-truenas-api-key-here"

# TrueNAS dataset path for NFS storage
truenas_dataset = "/mnt/SAS/RKE2"

# TrueNAS username (for reference/documentation)
truenas_user = "rke2"

# TrueNAS API protocol (https recommended)
truenas_protocol = "https"

# TrueNAS API port (443 for HTTPS, 80 for HTTP)
truenas_port = 443

# Allow insecure TLS (set to true if using self-signed certificate)
truenas_allow_insecure = false

# Storage class name for democratic-csi
csi_storage_class_name = "truenas-nfs"

# Make this storage class the default (only one default allowed per cluster)
csi_storage_class_default = true

# ============================================================================
# NOTES FOR PRODUCTION
# ============================================================================

# 1. Create secure API token:
#    ssh root@proxmox-node
#    pveum user token add terraform@pve terraform-prod --privsep=0
#
# 2. Use environment variables instead of hardcoding secrets:
#    export PROXMOX_VE_API_TOKEN="terraform@pve!terraform-prod=xxxxx"
#
# 3. For multi-team deployments, use Terraform workspaces:
#    terraform workspace new production
#    terraform workspace select production
#
# 4. Enable Terraform state encryption/locking in production:
#    - Use S3 backend with encryption
#    - Enable state locking with DynamoDB
#
# 5. Use separate tfvars files per environment:
#    terraform apply -var-file=environments/production.tfvars
#
# 6. Implement CI/CD pipeline for automated deployments:
#    - Use GitHub Actions or GitLab CI
#    - Require approval before apply
#    - Maintain audit trail of all changes
