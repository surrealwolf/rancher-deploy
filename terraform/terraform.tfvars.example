# Rancher Deploy - Terraform Variables Example
# 
# INSTRUCTIONS:
# 1. Copy this file: cp terraform.tfvars.example terraform.tfvars
# 2. Update values for your environment
# 3. Never commit terraform.tfvars to git (it's in .gitignore)
# 4. For team sharing, keep only terraform.tfvars.example in git
#
# This configuration creates:
# - 3 Rancher Manager nodes (VMs 401-403) - 40x range
# - 3 NPRD Apps server nodes (VMs 410-412) - 41x range
# - 3 NPRD Apps worker nodes (VMs 413-415) - 41x range
# - 3 PRD Apps server nodes (VMs 420-422) - 42x range
# - 3 PRD Apps worker nodes (VMs 423-425) - 42x range
# - 3 POC Apps server nodes (VMs 430-432) - 43x range
# - 3 POC Apps worker nodes (VMs 433-435) - 43x range
# - All from Ubuntu 24.04 LTS cloud images
# - Hybrid architecture: Servers (control plane) + Workers (workloads)
#
# VM ID Ranges:
#   manager:    40x (401-403)
#   nprd-apps:  41x (410-415)
#   prd-apps:   42x (420-425)
#   poc-apps:   43x (430-435)
#
# See CLOUD_IMAGE_SETUP.md for detailed instructions

# ============================================================================
# PROXMOX CONFIGURATION
# ============================================================================

proxmox_api_url = "https://192.168.1.10:8006"

# API user format: username@realm (typically root@pam for local auth)
proxmox_api_user = "root@pam"

# Token ID: Usually format is "tokenname" (not the full user!tokenid format)
proxmox_api_token_id = "terraform"

# Token secret: The actual token value (keep this secret!)
proxmox_api_token_secret = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Set to false in production with valid certificates
proxmox_tls_insecure = true

# Target Proxmox node where VMs will be created
proxmox_node = "pve"

# CPU type for VMs (e.g., qemu64, host, kvm64)
# Default: qemu64 (compatible with most hosts)
vm_cpu_type = "qemu64"

# ============================================================================
# UBUNTU CLOUD IMAGE CONFIGURATION (24.04 LTS Noble)
# ============================================================================

# Ubuntu cloud image URL - change to different release if needed:
# - 24.04 (Focal): https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img
# - 22.04 (Jammy): https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img
# - 24.04 (Noble): https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img
ubuntu_cloud_image_url = "https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"

# ============================================================================
# CLUSTER CONFIGURATION
# ============================================================================

clusters = {
  manager = {
    name           = "rancher-manager"
    node_count     = 3
    cpu_cores      = 4               # 2-4 minimum for Rancher
    memory_mb      = 8192            # 4GB minimum, 8GB recommended
    disk_size_gb   = 50               # Optimized: 50GB sufficient (can expand later if needed)
    domain         = "example.com"  # DNS domain for VMs
    ip_subnet      = "192.168.1"    # Base subnet
    ip_start_octet = 100             # VMs will be .100, .101, .102
    gateway        = "192.168.1.1"  # Network gateway
    dns_servers    = ["192.168.1.1"]
    storage        = "local-vm-zfs" # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id        = 1               # VLAN ID (1 for no VLAN, or your VLAN number)
  }

  nprd-apps = {
    name         = "nprd-apps"
    node_count   = 3    # Server nodes (control plane + etcd) - VMs 410-412
    worker_count = 3    # Worker nodes (workloads) - VMs 413-415
    cpu_cores    = 4    # Server CPU cores
    memory_mb    = 8192 # Server memory (4GB minimum, 8GB recommended)
    disk_size_gb = 80   # 80GB for control plane + etcd + container images
    # Worker node resources (optimized for workloads)
    worker_cpu_cores    = 4    # Worker CPU (same as server for consistency)
    worker_memory_mb    = 8192 # Worker RAM (same as server for consistency)
    worker_disk_size_gb = 80   # 80GB for workloads + container images
    domain              = "example.com"
    ip_subnet           = "192.168.1" # APPS CLUSTER SUBNET
    ip_start_octet      = 110         # Server VMs: .110, .111, .112 (VMs 410-412)
    # Worker VMs: .113, .114, .115 (VMs 413-415)
    gateway     = "192.168.1.1"
    dns_servers = ["192.168.1.1"] # Upstream DNS resolver
    storage     = "local-vm-zfs"  # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id     = 1               # VLAN ID for applications
  }

  prd-apps = {
    name         = "prd-apps"
    node_count   = 3    # Server nodes (control plane + etcd) - VMs 420-422
    worker_count = 3    # Worker nodes (workloads) - VMs 423-425
    cpu_cores    = 4    # Server CPU cores
    memory_mb    = 8192 # Server memory (4GB minimum, 8GB recommended)
    disk_size_gb = 80   # 80GB for control plane + etcd + container images
    # Worker node resources (optimized for workloads)
    worker_cpu_cores    = 4    # Worker CPU (same as server for consistency)
    worker_memory_mb    = 8192 # Worker RAM (same as server for consistency)
    worker_disk_size_gb = 80   # 80GB for workloads + container images
    domain              = "example.com"
    ip_subnet           = "192.168.1" # APPS CLUSTER SUBNET
    ip_start_octet      = 120         # Server VMs: .120, .121, .122 (VMs 420-422)
    # Worker VMs: .123, .124, .125 (VMs 423-425)
    gateway     = "192.168.1.1"
    dns_servers = ["192.168.1.1"] # Upstream DNS resolver
    storage     = "local-vm-zfs"  # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id     = 1               # VLAN ID for applications
  }

  poc-apps = {
    name         = "poc-apps"
    node_count   = 3    # Server nodes (control plane + etcd) - VMs 430-432
    worker_count = 3    # Worker nodes (workloads) - VMs 433-435
    cpu_cores    = 4    # Server CPU cores
    memory_mb    = 8192 # Server memory (4GB minimum, 8GB recommended)
    disk_size_gb = 80   # 80GB for control plane + etcd + container images
    # Worker node resources (optimized for workloads)
    worker_cpu_cores    = 4    # Worker CPU (same as server for consistency)
    worker_memory_mb    = 8192 # Worker RAM (same as server for consistency)
    worker_disk_size_gb = 80   # 80GB for workloads + container images
    domain              = "example.com"
    ip_subnet           = "192.168.1" # APPS CLUSTER SUBNET
    ip_start_octet      = 130         # Server VMs: .130, .131, .132 (VMs 430-432)
    # Worker VMs: .133, .134, .135 (VMs 433-435)
    gateway     = "192.168.1.1"
    dns_servers = ["192.168.1.1"] # Upstream DNS resolver
    storage     = "local-vm-zfs"  # VM storage (cloud image downloaded to 'local' then imported)
    vlan_id     = 1               # VLAN ID for applications
  }
}

# ============================================================================
# VM CONFIGURATION
# ============================================================================

# Starting VM IDs for clusters
vm_id_start_manager   = 401 # Manager cluster: VMs 401-403 (servers only) - 40x range
vm_id_start_nprd_apps = 410 # NPRD Apps cluster: VMs 410-412 (servers), 413-415 (workers) - 41x range
vm_id_start_prd_apps  = 420 # PRD Apps cluster: VMs 420-422 (servers), 423-425 (workers) - 42x range
vm_id_start_poc_apps  = 430 # POC Apps cluster: VMs 430-432 (servers), 433-435 (workers) - 43x range

# ============================================================================
# CERT-MANAGER CONFIGURATION
# ============================================================================

cert_manager_version = "v1.13.0"

# Enable Rancher deployment (default: true)
# Rancher installs on manager cluster after it's ready, then apps cluster deploys
# Set to false to deploy manager and apps clusters without Rancher
install_rancher = true

rancher_version = "v2.13.1"

# Initial admin password (change immediately after first login!)
rancher_password = "change-me-to-secure-password"

# Rancher manager hostname (should be accessible from application clusters)
rancher_hostname = "rancher.example.com"

# ============================================================================
# DOWNSTREAM CLUSTER REGISTRATION
# ============================================================================

# Automatic Rancher registration for downstream cluster (nprd-apps)
# Set to true to automatically register cluster with Rancher Manager
# Required: rancher_api_token must be set for this to work
register_downstream_cluster = true

# Downstream cluster ID in Rancher (format: c-abc123)
# DEPRECATED: This is now automatically fetched from Rancher API
# The cluster ID is extracted after Rancher creates/imports the cluster
# You can find it in Rancher UI under Cluster Management, but it's no longer required here
# Kept for backward compatibility but no longer used
# downstream_cluster_id = ""  # Leave empty - automatically populated from Rancher API

# Rancher API token for cluster management
# Obtain from Rancher: Account → API Tokens → Create API Token
# Token format: token-xxxxx:secret
# IMPORTANT: Keep this secret! Store in tfvars (not in git)
rancher_api_token = "token-xxxxx:your-secret-token"

# ============================================================================
# SSH CONFIGURATION
# ============================================================================

# Path to SSH private key for VM access
# The key should match the public key configured in your Proxmox user account
ssh_private_key = "~/.ssh/id_rsa"

# ============================================================================
# DOWNSTREAM CLUSTER REGISTRATION
# ============================================================================

# Name of the downstream cluster to register with Rancher Manager
# Defaults to first non-manager cluster from clusters map (e.g., "nprd-apps")
# Change this to register a different cluster or leave empty for auto-detection
# downstream_cluster_name = "nprd-apps"  # Optional: specify explicitly

# Enable automatic registration of downstream cluster with Rancher Manager
# When true, Terraform will:
# 1. Create the cluster resource in Rancher automatically via API
# 2. Register all nodes with Rancher Manager automatically
# No manual Rancher UI steps required!
register_downstream_cluster = true

# ============================================================================
# CLUSTER HOSTNAME AND PRIMARY IP CONFIGURATION
# ============================================================================
# These are used in RKE2 TLS certificate generation (tls-san values)
# Must match your DNS configuration

manager_cluster_hostname   = "manager.example.com"
manager_cluster_primary_ip = "192.168.1.100"
manager_cluster_aliases    = ["rancher.example.com"] # Alias for Rancher UI access

nprd_apps_cluster_hostname   = "nprd-apps.example.com"
nprd_apps_cluster_primary_ip = "192.168.1.110"
nprd_apps_cluster_aliases    = ["nprd.example.com"] # Alias for nprd apps cluster access

prd_apps_cluster_hostname   = "prd-apps.example.com"
prd_apps_cluster_primary_ip = "192.168.1.120"
prd_apps_cluster_aliases    = ["prd.example.com"] # Alias for prd apps cluster access

poc_apps_cluster_hostname   = "poc-apps.example.com"
poc_apps_cluster_primary_ip = "192.168.1.130"
poc_apps_cluster_aliases    = ["poc.example.com"] # Alias for poc apps cluster access

# IP address of Rancher Manager ingress (for downstream cluster system-agent registration)
# Set to one of the manager node IPs (usually the primary node IP)
rancher_manager_ip = "192.168.1.100"

# ============================================================================
# TRUENAS / DEMOCRATIC CSI CONFIGURATION
# ============================================================================
# Configuration for democratic-csi storage driver with TrueNAS
# These values are used to generate Helm values for democratic-csi installation

# TrueNAS hostname or IP address
truenas_host = "tn.dataknife.net"

# TrueNAS API key (obtain from TrueNAS UI: System → API Keys → Add)
# Format: token-id-token-secret (e.g., 1-xxxxxxxxxxxxx)
truenas_api_key = "your-truenas-api-key-here"

# TrueNAS dataset path for NFS storage
truenas_dataset = "/mnt/SAS/RKE2"

# TrueNAS username (for reference/documentation)
truenas_user = "rke2"

# TrueNAS API protocol (https recommended)
truenas_protocol = "https"

# TrueNAS API port (443 for HTTPS, 80 for HTTP)
truenas_port = 443

# Allow insecure TLS (set to true if using self-signed certificate)
truenas_allow_insecure = false

# Storage class name for democratic-csi
csi_storage_class_name = "truenas-nfs"

# Make this storage class the default (only one default allowed per cluster)
csi_storage_class_default = true

# ============================================================================
# ENVOY GATEWAY CONFIGURATION
# ============================================================================

# Install Envoy Gateway on downstream clusters (recommended for Gateway API)
install_envoy_gateway = true

# Gateway API CRDs version (default: v1.1.0)
# Envoy Gateway 1.6.1 uses Gateway API 1.4.1 internally
# v1.1.0 is the latest stable Gateway API CRDs and is compatible
# Alternatively, set to "helm" to let Envoy Gateway Helm chart install CRDs automatically
gateway_api_version = "v1.1.0"

# Envoy Gateway Helm chart version (default: v1.6.1)
# Check latest version: https://github.com/envoyproxy/gateway/releases
envoy_gateway_version = "v1.6.1"

# ============================================================================
# OPENSEARCH OPERATOR CONFIGURATION
# ============================================================================

# OpenSearch Kubernetes Operator Helm chart version (default: 2.8.0)
# Check latest version: https://github.com/opensearch-project/opensearch-k8s-operator/releases
opensearch_operator_version = "2.8.0"

# MongoDB Community Operator Helm chart version (default: 0.13.0)
# Check latest version: https://github.com/mongodb/mongodb-kubernetes-operator/releases
mongodb_operator_version = "0.13.0"

# CloudNativePG Operator version (default: 1.28.0)
# Installed via manifest, not Helm
# Check latest version: https://github.com/cloudnative-pg/cloudnative-pg/releases
cloudnativepg_operator_version = "1.28.0"

# GitHub Actions Runner Controller (ARC) Helm chart version (default: 0.13.1)
# Check latest version: https://github.com/actions/actions-runner-controller/releases
github_arc_controller_version = "0.13.1"

# ============================================================================
# NOTES FOR PRODUCTION
# ============================================================================

# 1. Create secure API token:
#    ssh root@proxmox-node
#    pveum user token add terraform@pve terraform-prod --privsep=0
#
# 2. Use environment variables instead of hardcoding secrets:
#    export PROXMOX_VE_API_TOKEN="terraform@pve!terraform-prod=xxxxx"
#
# 3. For multi-team deployments, use Terraform workspaces:
#    terraform workspace new production
#    terraform workspace select production
#
# 4. Enable Terraform state encryption/locking in production:
#    - Use S3 backend with encryption
#    - Enable state locking with DynamoDB
#
# 5. Use separate tfvars files per environment:
#    terraform apply -var-file=environments/production.tfvars
#
# 6. Implement CI/CD pipeline for automated deployments:
#    - Use GitHub Actions or GitLab CI
#    - Require approval before apply
#    - Maintain audit trail of all changes
